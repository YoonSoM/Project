{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Unet_binary.ipynb",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!gdown 1z70VmSxCOPNvCc-wna8W08wzvguESYqL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aESBKmSIPS2w",
        "outputId": "56ccc913-6a51-466c-e013-84f74172fbf0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1z70VmSxCOPNvCc-wna8W08wzvguESYqL\n",
            "To: /content/NPYdata_0729.zip\n",
            "100% 141M/141M [00:01<00:00, 103MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/NPYdata_0729.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1iMyVpVPXzQ",
        "outputId": "72950480-a670-4a51-ca73-66615da117c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/NPYdata_0729.zip\n",
            "   creating: data/\n",
            "   creating: data/label3/\n",
            "  inflating: data/label3/label3_mask.npy  \n",
            "  inflating: data/label3/label3_image.npy  \n",
            "   creating: data/label4/\n",
            "  inflating: data/label4/label4_mask.npy  \n",
            "  inflating: data/label4/label4_image.npy  \n",
            "   creating: data/label2/\n",
            "  inflating: data/label2/label2_mask.npy  \n",
            "  inflating: data/label2/label2_image.npy  \n",
            "   creating: data/label1/\n",
            "  inflating: data/label1/label1_mask.npy  \n",
            "  inflating: data/label1/label1_image.npy  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "R7iEwW-OYpqy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " data = np.ones(shape =(188,))"
      ],
      "metadata": {
        "id": "c-M8zcrMP6uj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save = ('labels.npy', data)"
      ],
      "metadata": {
        "id": "yP8P2AZiQnaP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = np.load('/content/data/label1/label1_image.npy')\n",
        "data2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrniND_0QvTP",
        "outputId": "826653a0-9e77-483f-b067-71cfc3c4d91e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[249, 249, 249, ..., 106, 106, 106],\n",
              "        [242, 242, 242, ..., 106, 106, 106],\n",
              "        [246, 246, 246, ..., 107, 106, 106],\n",
              "        ...,\n",
              "        [197, 195, 192, ..., 197, 197, 197],\n",
              "        [201, 198, 195, ..., 197, 197, 197],\n",
              "        [208, 204, 200, ..., 197, 197, 197]],\n",
              "\n",
              "       [[ 15,  15,  14, ..., 151, 142, 131],\n",
              "        [ 15,  15,  14, ..., 151, 141, 131],\n",
              "        [ 14,  14,  14, ..., 151, 140, 131],\n",
              "        ...,\n",
              "        [ 83,  88,  91, ..., 108, 107, 111],\n",
              "        [ 84,  86,  90, ..., 108, 105, 107],\n",
              "        [ 85,  84,  86, ..., 110, 105, 106]],\n",
              "\n",
              "       [[106, 106, 106, ..., 127, 127, 127],\n",
              "        [106, 106, 106, ..., 127, 127, 127],\n",
              "        [106, 106, 106, ..., 128, 128, 128],\n",
              "        ...,\n",
              "        [ 27,  27,  26, ...,  42,  42,  42],\n",
              "        [ 27,  26,  26, ...,  44,  42,  41],\n",
              "        [ 26,  26,  26, ...,  55,  53,  52]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[197, 197, 199, ..., 101, 101, 101],\n",
              "        [198, 199, 200, ..., 102, 101, 101],\n",
              "        [201, 201, 201, ..., 102, 102, 101],\n",
              "        ...,\n",
              "        [179, 180, 174, ...,  64,  62,  61],\n",
              "        [185, 180, 169, ...,  64,  62,  61],\n",
              "        [186, 177, 164, ...,  63,  62,  60]],\n",
              "\n",
              "       [[ 34,  34,  34, ..., 179, 177, 176],\n",
              "        [ 34,  34,  35, ..., 178, 176, 175],\n",
              "        [ 35,  35,  35, ..., 177, 175, 174],\n",
              "        ...,\n",
              "        [ 35,  35,  35, ..., 163, 160, 157],\n",
              "        [ 35,  35,  35, ..., 160, 158, 155],\n",
              "        [ 35,  35,  35, ..., 158, 155, 153]],\n",
              "\n",
              "       [[142, 153, 159, ..., 139, 133, 131],\n",
              "        [136, 149, 158, ..., 138, 133, 131],\n",
              "        [128, 142, 155, ..., 137, 133, 131],\n",
              "        ...,\n",
              "        [131, 131, 131, ...,  72,  69,  67],\n",
              "        [131, 131, 131, ...,  72,  69,  66],\n",
              "        [131, 131, 131, ...,  72,  69,  66]]], dtype=int16)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "\n",
        "for dirname, subdir, filenames in tqdm(os.walk('/kaggle/input'), total = 3):\n",
        "    for filename in filenames:\n",
        "        filepath = (os.path.join(dirname, filename))\n",
        "        print(\"loading from\", filepath)\n",
        "        if \"images\" in filename:\n",
        "            images = np.load(filepath, allow_pickle = True)\n",
        "        elif \"masks\" in filename:\n",
        "            masks = np.load(filepath, allow_pickle = True)\n",
        "        else:\n",
        "            labels = np.load(filepath, allow_pickle = True)\n",
        "        \n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66cBVaZVIW4o",
        "outputId": "43fd4f19-9608-4a57-c342-7568a78bfdd3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Exploring data</h3>"
      ],
      "metadata": {
        "id": "t224VSumIW4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_number = 9\n",
        "fig, ax = plt.subplots(3, 3, figsize=(15, 15))\n",
        "for i in range(sample_number):\n",
        "    subplot = ax[int(i//3), int(i%3)]\n",
        "    subplot.imshow(images[i])\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "7EPqjaRAIW4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = np.column_stack((images, masks, labels))\n",
        "print(trainset.shape)\n",
        "# 3064x3x512x512\n",
        "from sklearn.model_selection import train_test_split\n",
        "trainset, testset = train_test_split(trainset, test_size = 0.1)\n",
        "trainset, validset = train_test_split(trainset, test_size = 0.05)\n",
        "print('size of trainset, testset and validset is',len(trainset), len(testset), len(validset))"
      ],
      "metadata": {
        "trusted": true,
        "id": "kLqTlbzAIW4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"meningioma\", \"glioma\", \"pituitary tumor\"] \n",
        "\n",
        "fig, ax = plt.subplots(3, 3, figsize=(15, 15))\n",
        "for i in range(sample_number):\n",
        "    subplot = ax[int(i//3), int(i%3)]\n",
        "    subplot.imshow(trainset[i][0])\n",
        "    subplot.set_title(classes[trainset[i][2]-1])\n",
        "    subplot.imshow(trainset[i][1], cmap='gray', alpha=0.2)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ytuc61h4IW4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "batch_size = 32\n",
        "workers = 4\n",
        "print(\"device available: \", device)"
      ],
      "metadata": {
        "trusted": true,
        "id": "y3gd4Wt0IW4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Helper functions</h3>"
      ],
      "metadata": {
        "id": "aqtdkYx6IW4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transform\n",
        "import torchvision.transforms.functional as TF\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "random.seed(123)\n",
        "\n",
        "class Resize(object):\n",
        "    def __init__(self, size=216):\n",
        "        self.size = size\n",
        "    \n",
        "    def __call__(self, sample):\n",
        "        image, mask, label = sample\n",
        "        image = TF.resize(image, size=(self.size, self.size), interpolation=Image.NEAREST)\n",
        "        mask = TF.resize(mask, size = (self.size, self.size), interpolation=Image.NEAREST)\n",
        "        return image, mask, label\n",
        "    \n",
        "\n",
        "class toPIL(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def __call__(self, sample):\n",
        "        image, mask, label = sample\n",
        "        image = TF.to_pil_image(image.astype(np.float32))\n",
        "        mask = TF.to_pil_image(mask.astype(np.float32))\n",
        "        return image, mask, label\n",
        "\n",
        "    \n",
        "class toTensor(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def __call__(self, sample):\n",
        "        image, mask, label = sample\n",
        "        image = TF.to_tensor(image)\n",
        "        mask = TF.to_tensor(mask)\n",
        "        return image, mask, label\n",
        "    \n",
        "    \n",
        "class Rotate(object):\n",
        "    def __init__(self, p):\n",
        "        self.p = p\n",
        "        \n",
        "    def __call__(self, sample):\n",
        "        image, mask, label = sample\n",
        "        if random.random() < self.p:\n",
        "            angle = random.randint(-30, 30)\n",
        "            image = TF.rotate(image, angle)\n",
        "            mask = TF.rotate(mask, angle)\n",
        "        return image, mask, label\n",
        "    \n",
        "    \n",
        "class Flip(object):\n",
        "    def __init__(self, p):\n",
        "        self.p = p\n",
        "        \n",
        "    def __call__(self, sample):\n",
        "        image, mask, label = sample\n",
        "        if random.random() < self.p:\n",
        "            image = TF.hflip(image)\n",
        "            mask = TF.hflip(mask)\n",
        "        return image, mask, label\n",
        "    \n",
        "\n",
        "class Crop(object):\n",
        "    def __init__(self, p):\n",
        "        self.p = p\n",
        "    \n",
        "    def __call__(self, sample):\n",
        "        image, mask, label = sample\n",
        "        if random.random() < self.p:\n",
        "            size = image.size[1]\n",
        "            crop_size = random.randint(int(size*0.5), size)\n",
        "            image = TF.center_crop(image, output_size=crop_size)\n",
        "            mask = TF.center_crop(mask, output_size=crop_size)\n",
        "        return image, mask, label\n",
        "    \n",
        "    \n",
        "class Padding(object):\n",
        "    def __init__(self, size):\n",
        "        super(Padding, self).__init__()\n",
        "        self.size = size\n",
        "        \n",
        "    def __call__(self, sample):\n",
        "        image, mask, label = sample\n",
        "        w, h = image.size\n",
        "        if (w < self.size):\n",
        "            image = TF.pad(image, padding = (self.size-w)//2, fill=0)\n",
        "            mask = TF.pad(mask, padding = (self.size-w)//2, fill=0)\n",
        "        return image, mask, label\n",
        "    \n",
        "class ZoomOut(object):\n",
        "    def __init__(self, p):\n",
        "        super(ZoomOut, self).__init__()\n",
        "        self.p = p\n",
        "        \n",
        "    def __call__(self, sample):\n",
        "        image, mask, label = sample\n",
        "        w, h = image.size\n",
        "        if (random.random() < self.p):\n",
        "            resize = Resize(random.randint(int(0.5*w), int(.95*w)))\n",
        "            pad = Padding(w)\n",
        "            sample = resize(sample)\n",
        "            sample = pad(sample)\n",
        "        return sample\n",
        "    \n",
        "    \n",
        "def toNumpy(sample):\n",
        "    image, mask, label = sample\n",
        "    image = np.array(image)\n",
        "    mask = np.array(mask)\n",
        "    return image, mask, label\n",
        "\n",
        "\n",
        "def normalize(sample):\n",
        "    image, mask, label = sample\n",
        "    m, s = np.mean(image), np.std(image)\n",
        "    image = (image - m)/s\n",
        "    return image, mask, label\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "tMeuqSgxIW4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getTransform(rotate, crop, flip, zoom):\n",
        "    return transform.Compose([\n",
        "        Rotate(rotate),\n",
        "        ZoomOut(zoom),\n",
        "        Crop(crop),\n",
        "        Flip(flip),\n",
        "    ])\n",
        "\n",
        "func = toPIL()\n",
        "testimg = func(trainset[0])\n",
        "trans = getTransform(rotate=1, crop=1, flip=1, zoom=1 )\n",
        "test = trans(testimg)\n",
        "fig, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
        "ax[0, 0].imshow(np.array(testimg[0]))\n",
        "ax[0, 0].set_title(\"original image\")\n",
        "ax[0, 1].imshow(np.array(test[0]))\n",
        "ax[0, 1].set_title('transformed image')\n",
        "ax[1, 0].imshow(np.array(testimg[1]), cmap='gray')\n",
        "ax[1, 0].set_title('original mask')\n",
        "ax[1, 1].imshow(np.array(test[1]), cmap='gray')\n",
        "ax[1, 1].set_title('transformed mask')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "xi-JC35UIW4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Dataset class and DataLoader</h3>"
      ],
      "metadata": {
        "id": "9Q73-h1BIW40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "input_size = 128\n",
        "class MRIDataset(Dataset):\n",
        "    def __init__(self, data, train=True, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "        self.train = train\n",
        "        self.data = [normalize(i) for i in self.data]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        sample = self.data[i]\n",
        "        \n",
        "        to_pil = toPIL()\n",
        "        sample = to_pil(sample)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(sample)\n",
        "            \n",
        "        resize_func = Resize(input_size)\n",
        "        sample = resize_func(sample)\n",
        "        sample = toNumpy(sample)\n",
        "        \n",
        "        image, mask, label = sample\n",
        "        target_label = np.zeros((3, 1, 1))\n",
        "        target_label[label-1, 0, 0] = 1\n",
        "        label = target_label\n",
        "                \n",
        "        image = torch.from_numpy(image.astype(np.float32))\n",
        "        mask = torch.from_numpy(mask.astype(np.float32))\n",
        "        label = torch.from_numpy(label.astype(np.float32))\n",
        "        \n",
        "        image = torch.unsqueeze(image, dim = 0)\n",
        "        mask = torch.unsqueeze(mask, dim = 0)\n",
        "\n",
        "        \n",
        "        return image, mask, label\n",
        "    \n",
        "trainset = MRIDataset(trainset, transform = getTransform(.8, .8, .8, .7)) \n",
        "testset  = MRIDataset(testset)\n",
        "validset = MRIDataset(validset)"
      ],
      "metadata": {
        "trusted": true,
        "id": "vmvR_cxYIW40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getDataLoader(dataset, batch = batch_size):\n",
        "    dataloader = DataLoader(\n",
        "        dataset, \n",
        "        batch_size = batch,\n",
        "        shuffle = True,\n",
        "        num_workers = workers\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "testLoader  = getDataLoader(testset)\n",
        "trainLoader = getDataLoader(trainset)\n",
        "validLoader = getDataLoader(validset)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZVfOLJDSIW41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Define model</h3>"
      ],
      "metadata": {
        "id": "Igjdh5QjIW41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "in_features = 128\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, in_channels = 1, out_channel = 1, init_features=in_features):\n",
        "        super(Unet, self).__init__()\n",
        "        self.in_channels = 1\n",
        "        self.out_channels = 3\n",
        "        self.init_features = init_features\n",
        "        \n",
        "        self.encoder1 = self.__block__(inchannels = in_channels,\n",
        "                                      outchannels = init_features)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.encoder2 = self.__block__(inchannels = init_features,\n",
        "                                      outchannels = 2*init_features)\n",
        "        self.encoder3 = self.__block__(inchannels = 2*init_features,\n",
        "                                      outchannels = 4*init_features)\n",
        "        self.encoder4 = self.__block__(inchannels = 4*init_features,\n",
        "                                      outchannels = 8*init_features)\n",
        "        self.bottle   = self.__block__(inchannels = 8*init_features,\n",
        "                                      outchannels = 16*init_features)\n",
        "        self.upconv4  = nn.ConvTranspose2d(\n",
        "            in_channels = 16*init_features, out_channels = 8 * init_features,\n",
        "            kernel_size = 2, stride = 2\n",
        "        )\n",
        "        self.decoder4 = self.__block__(inchannels = 16*init_features,\n",
        "                                      outchannels = 8*init_features)\n",
        "        self.upconv3  = nn.ConvTranspose2d(\n",
        "            in_channels = 8 * init_features, out_channels = 4 * init_features,\n",
        "            kernel_size = 2, stride = 2\n",
        "        )\n",
        "        self.decoder3 = self.__block__(inchannels = 8*init_features,\n",
        "                                      outchannels = 4*init_features)\n",
        "        self.upconv2 = nn.ConvTranspose2d(\n",
        "            in_channels = 4 * init_features, out_channels = 2 * init_features,\n",
        "            kernel_size = 2, stride = 2\n",
        "        )\n",
        "        self.decoder2 = self.__block__(inchannels = 4*init_features,\n",
        "                                      outchannels = 2*init_features)\n",
        "        self.upconv1 = nn.ConvTranspose2d(\n",
        "            in_channels = 2 * init_features, out_channels = init_features,\n",
        "            kernel_size = 2, stride = 2\n",
        "        )\n",
        "        self.decoder1 = self.__block__(inchannels = 2*init_features, \n",
        "                                      outchannels = init_features)\n",
        "        self.final = nn.Conv2d(in_channels = init_features, out_channels = out_channel, kernel_size=1)\n",
        "        \n",
        "        # self.sub_final = self.__fc__(16*init_features)\n",
        "        \n",
        "            \n",
        "    def forward(self, x):\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool(enc1))\n",
        "        enc3 = self.encoder3(self.pool(enc2))\n",
        "        enc4 = self.encoder4(self.pool(enc3))\n",
        "        \n",
        "        bottom = self.bottle(self.pool(enc4))\n",
        "        pred_label = self.sub_final(bottom)\n",
        "        \n",
        "        dec = self.upconv4(bottom)\n",
        "        dec = torch.cat((dec, enc4), dim=1)\n",
        "        dec = self.decoder4(dec)\n",
        "        \n",
        "        dec = self.upconv3(dec)\n",
        "        dec = torch.cat((dec, enc3), dim=1)\n",
        "        dec = self.decoder3(dec)\n",
        "        \n",
        "        dec = self.upconv2(dec)\n",
        "        dec = torch.cat((dec, enc2), dim=1)\n",
        "        dec = self.decoder2(dec)\n",
        "        \n",
        "        dec = self.upconv1(dec)\n",
        "        dec = torch.cat((dec, enc1), dim=1)\n",
        "        dec = self.decoder1(dec)\n",
        "        \n",
        "        dec = self.final(dec)\n",
        "        \n",
        "        return torch.sigmoid(dec), pred_label\n",
        "        \n",
        "    class __block__(nn.Module):\n",
        "        def __init__(self, inchannels, outchannels):\n",
        "            super().__init__()\n",
        "   \n",
        "            self.conv = nn.Sequential( \n",
        "                nn.Conv2d(\n",
        "                    in_channels=inchannels,\n",
        "                    out_channels=outchannels,\n",
        "                    kernel_size=3,\n",
        "                    padding=1,\n",
        "                    bias=False,\n",
        "                ),\n",
        "                nn.BatchNorm2d(num_features=outchannels)\n",
        "            )\n",
        "            self.block = nn.Sequential(\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(\n",
        "                    in_channels=outchannels,\n",
        "                    out_channels=outchannels,\n",
        "                    kernel_size=3,\n",
        "                    padding=1,\n",
        "                    bias=False,\n",
        "                ),\n",
        "                nn.BatchNorm2d(num_features=outchannels)\n",
        "            )\n",
        "            self.relu = nn.ReLU()\n",
        "            \n",
        "        \n",
        "        def forward(self, x):\n",
        "            x = self.conv(x)\n",
        "            return self.relu(self.block(x) + x)\n",
        "\n",
        "    # class __fc__(nn.Module):\n",
        "    #     def __init__(self, in_features):\n",
        "    #         super().__init__()\n",
        "    #         self.block = nn.Sequential(\n",
        "    #             nn.Conv2d(\n",
        "    #                 in_channels=in_features,\n",
        "    #                 out_channels=in_features//2,\n",
        "    #                 kernel_size=2,\n",
        "    #                 stride=2,\n",
        "    #                 bias=False,\n",
        "    #             ),\n",
        "    #             nn.BatchNorm2d(num_features=in_features//2),\n",
        "    #             nn.ReLU(),\n",
        "    #             nn.Conv2d(\n",
        "    #                 in_channels=in_features//2,\n",
        "    #                 out_channels=in_features//4,\n",
        "    #                 kernel_size=2,\n",
        "    #                 stride=2,\n",
        "    #                 bias=False,\n",
        "    #             ),\n",
        "    #             nn.BatchNorm2d(num_features=in_features//4),\n",
        "    #             nn.ReLU(),\n",
        "    #             nn.Conv2d(\n",
        "    #                 in_channels=in_features//4,\n",
        "    #                 out_channels=3,\n",
        "    #                 kernel_size=2,\n",
        "    #                 stride=2,\n",
        "    #                 bias=False,\n",
        "    #             ),\n",
        "    #             nn.Softmax(dim=1)\n",
        "    #         )\n",
        "            \n",
        "        def forward(self,x) :\n",
        "            return self.block(x)"
      ],
      "metadata": {
        "trusted": true,
        "id": "19hIZEi-IW42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Loss function</h3>"
      ],
      "metadata": {
        "id": "sjA6e-hZIW42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coef(y_pred, y_true):\n",
        "    smooth = 1e-3\n",
        "    numerator =   (2 * ( y_pred * y_true ).sum() + smooth)\n",
        "    denominator = ( y_pred.sum() + y_true.sum()  + smooth)\n",
        "    return numerator / denominator\n",
        "\n",
        "def dice_loss(y_pred, y_true):\n",
        "    return 1. - dice_coef(y_pred, y_true)\n",
        "\n",
        "class dice(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(dice, self).__init__()\n",
        "        \n",
        "    def forward(self, y_pred, y_true):\n",
        "        return dice_loss(y_pred, y_true)\n",
        "\n",
        "def dsc(y_pred, y_true):\n",
        "    smooth = 1e-3\n",
        "    y_pred, y_true = torch.round(y_pred), torch.round(y_true)\n",
        "    numerator = (2 * ( y_pred * y_true ).sum() + smooth)\n",
        "    denominator = ( y_pred.sum() + y_true.sum()  + smooth)\n",
        "    return numerator / denominator\n",
        "\n",
        "class bce_dice_loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(bce_dice_loss, self).__init__()\n",
        "        self.loss = nn.BCELoss()\n",
        "        \n",
        "    def forward(self, mask_pred, label_pred, mask_true, label_true):\n",
        "        return dice_loss(mask_pred, mask_true) + self.loss(label_pred.squeeze(), label_true.squeeze())"
      ],
      "metadata": {
        "trusted": true,
        "id": "AfP8UvEXIW43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Training model</h3>"
      ],
      "metadata": {
        "id": "XrGHQwJKIW43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "unet = Unet()\n",
        "unet.to(device)\n",
        "adam = optim.Adam(unet.parameters(), lr=1e-3)\n",
        "scheduler = StepLR(adam, step_size=20, gamma=0.1)\n",
        "epochs = 50\n",
        "loss_func = bce_dice_loss()\n",
        "train_loss_history = []\n",
        "valid_loss_history = []\n",
        "best_valid = 1e3\n",
        "\n",
        "for epoc in range(epochs):\n",
        "    print('------')\n",
        "    print('epoch:', epoc + 1,'/',epochs)\n",
        "    train_loss = []\n",
        "    valid_loss = []\n",
        "    \n",
        "    # train \n",
        "    unet.train()\n",
        "    pbar = tqdm(trainLoader, total=int(len(trainset)//batch_size),position=0, leave=True)\n",
        "    \n",
        "    for data, mask, label in pbar:\n",
        "        data, mask, label = data.to(device), mask.to(device), label.to(device)\n",
        "        mask_pred, label_pred = unet(data)\n",
        "        adam.zero_grad()\n",
        "        loss = loss_func(mask_pred, label_pred, mask, label)\n",
        "        train_loss.append(loss.item())\n",
        "        loss.backward()\n",
        "        adam.step()\n",
        "    train_loss_history.append(np.mean(train_loss))\n",
        "    \n",
        "    \n",
        "    # valid \n",
        "    with torch.no_grad():\n",
        "        unet.eval()\n",
        "        for data, mask, label in validLoader:\n",
        "            data, mask, label = data.to(device), mask.to(device), label.to(device)\n",
        "            mask_pred, label_pred = unet(data)\n",
        "            loss = loss_func(mask_pred, label_pred, mask, label)\n",
        "            valid_loss.append(loss.item())\n",
        "\n",
        "    valid_loss_history.append(np.mean(valid_loss))\n",
        "    print('train loss:', train_loss_history[-1])\n",
        "    print('valid loss:', valid_loss_history[-1])\n",
        "    \n",
        "    # save better model\n",
        "    if (best_valid > valid_loss_history[-1]):\n",
        "        best_valid = valid_loss_history[-1]\n",
        "        torch.save(unet.state_dict(), os.path.join('./', 'unet.pt'))\n",
        "        \n",
        "    scheduler.step()\n",
        "\n",
        "    \n",
        "print('best validation loss:', best_valid)\n",
        "\n",
        "# load model\n",
        "state_dict = torch.load(os.path.join('./', 'unet.pt'))\n",
        "unet.load_state_dict(state_dict);\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "byM5PvExIW43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save progess\n",
        "from numpy import savetxt\n",
        "savetxt('./train_loss_history.csv', train_loss_history, delimiter=',')\n",
        "savetxt('./valid_loss_history.csv', valid_loss_history, delimiter=',')"
      ],
      "metadata": {
        "trusted": true,
        "id": "9ElS0REtIW44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>plot training progress</h3>"
      ],
      "metadata": {
        "id": "kjXGqqQCIW44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(epochs)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, train_loss_history, label='train loss', lw=3, color='blue')\n",
        "plt.plot(x, valid_loss_history, label='valid loss', lw=3, color='red')\n",
        "plt.xlabel('epoch', fontsize=13)\n",
        "plt.ylabel('loss',fontsize=13)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "eFadendhIW44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>testing model</h3>"
      ],
      "metadata": {
        "id": "z9oZEHT3IW45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unet.eval()\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_loss_dsc = []\n",
        "    for img, mask, label in testLoader:\n",
        "        img, mask, label = img.to(device), mask.to(device), label.to(device)\n",
        "        mask_pred, label_pred = unet(img)\n",
        "        class_pred = torch.argmax(label_pred, dim=1)\n",
        "        class_true = torch.argmax(label, dim=1)\n",
        "        correct = torch.flatten(class_pred == class_true)\n",
        "        \n",
        "        \n",
        "        mask_pred = torch.unsqueeze(torch.unsqueeze(torch.unsqueeze(correct, dim=1),dim=2), dim=3) * mask_pred\n",
        "        loss = dsc(mask_pred, mask)\n",
        "        test_loss_dsc.append(loss.item())\n",
        "        \n",
        "        \n",
        "    print(\"test's dsc:\", np.mean(test_loss_dsc))"
      ],
      "metadata": {
        "trusted": true,
        "id": "bRD8DZVXIW45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Showing some results</h3>"
      ],
      "metadata": {
        "id": "Z3HqgF4TIW45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def showRes(img, mask_pred, label_pred, mask, label):\n",
        "    img, mask = img.detach().detach().cpu().numpy(), mask.detach().cpu().numpy()\n",
        "    label = label.detach().cpu().numpy()\n",
        "    mask_pred, label_pred = mask_pred.detach().cpu().numpy(), label_pred.cpu().numpy()\n",
        "    mask_pred = np.round(mask_pred)\n",
        "    label_pred = np.argmax(label_pred, axis=0)\n",
        "    label_pred = label_pred.flatten()\n",
        "    \n",
        "    label = np.argmax(label, axis=0)\n",
        "    label = label.flatten()\n",
        "    \n",
        "    \n",
        "    fig, ax = plt.subplots(2, 2, figsize=(6, 6))\n",
        "    ax[0, 0].imshow(img[0])\n",
        "    ax[0, 0].set_title('image')\n",
        "    ax[0, 1].imshow(mask[0], cmap='gray')\n",
        "    ax[0, 1].set_title('gt ' + classes[label[0]])\n",
        "    \n",
        "    ax[1, 0].imshow(img[0])\n",
        "    ax[1, 0].set_title('image')\n",
        "    ax[1, 1].imshow(mask_pred[0], cmap='gray')\n",
        "    ax[1, 1].set_title('pred ' + classes[label_pred[0]])\n",
        " "
      ],
      "metadata": {
        "trusted": true,
        "id": "5n7mHf7XIW45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_batch(img, pred, mask, lable):\n",
        "    mask_pred, label_pred = pred\n",
        "    for i in range(img.size()[0]):\n",
        "        showRes(img[i], mask_pred[i], label_pred[i].squeeze(), mask[i], label[i].squeeze())"
      ],
      "metadata": {
        "trusted": true,
        "id": "CPnRpvpKIW46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for img, mask, label in testLoader:\n",
        "        img, mask, label = img.to(device), mask.to(device), label.to(device)\n",
        "        pred = unet(img)\n",
        "        show_batch(img, pred, mask, label)\n",
        "        break\n",
        "        "
      ],
      "metadata": {
        "trusted": true,
        "id": "8WmzTxbmIW46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "25HSJi25IW46"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}